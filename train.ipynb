{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f43658d0-e0f1-4db9-903d-a3851b7fc714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练任务...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded: s3://sagemaker-ap-northeast-1-034362076319/train/train_processed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-11-08-15-54-35-976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 15:54:38 Starting - Starting the training job...\n",
      "2024-11-08 15:55:03 Starting - Preparing the instances for training...\n",
      "2024-11-08 15:55:35 Downloading - Downloading input data...\n",
      "2024-11-08 15:55:55 Downloading - Downloading the training image........................\n",
      "2024-11-08 15:59:59 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:15,238 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:15,257 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:15,270 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:15,272 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:17,311 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:17,360 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:17,393 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:17,407 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 5,\n",
      "        \"eval_batch_size\": 32,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"model_name\": \"thu-coai/roberta-base-cold\",\n",
      "        \"train_batch_size\": 32,\n",
      "        \"warmup_steps\": 500,\n",
      "        \"weight_decay\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-11-08-15-54-35-976\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-1-034362076319/huggingface-pytorch-training-2024-11-08-15-54-35-976/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"eval_batch_size\":32,\"learning_rate\":3e-05,\"model_name\":\"thu-coai/roberta-base-cold\",\"train_batch_size\":32,\"warmup_steps\":500,\"weight_decay\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-1-034362076319/huggingface-pytorch-training-2024-11-08-15-54-35-976/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"eval_batch_size\":32,\"learning_rate\":3e-05,\"model_name\":\"thu-coai/roberta-base-cold\",\"train_batch_size\":32,\"warmup_steps\":500,\"weight_decay\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-11-08-15-54-35-976\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-1-034362076319/huggingface-pytorch-training-2024-11-08-15-54-35-976/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--eval_batch_size\",\"32\",\"--learning_rate\",\"3e-05\",\"--model_name\",\"thu-coai/roberta-base-cold\",\"--train_batch_size\",\"32\",\"--warmup_steps\",\"500\",\"--weight_decay\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=thu-coai/roberta-base-cold\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --epochs 5 --eval_batch_size 32 --learning_rate 3e-05 --model_name thu-coai/roberta-base-cold --train_batch_size 32 --warmup_steps 500 --weight_decay 0.01\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:17,407 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-11-08 16:00:17,407 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m{'learning_rate': 2e-05, 'train_batch_size': 16, 'eval_batch_size': 16, 'epochs': 6.0, 'weight_decay': 0.01, 'warmup_steps': 100, 'model_name': 'thu-coai/roberta-base-cold', 'max_length': 128, 'early_stopping_patience': 3, 'dropout_rate': 0.3, 'label_smoothing': 0.1, 'max_grad_norm': 0.5, 'warmup_ratio': 0.15, 'gradient_accumulation_steps': 2}\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 11038 examples [00:00, 1451717.65 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2856 examples [00:00, 798116.61 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/11038 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 2000/11038 [00:00<00:00, 11967.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 4000/11038 [00:00<00:00, 12411.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 6000/11038 [00:00<00:00, 12514.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 8000/11038 [00:00<00:00, 12606.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 10000/11038 [00:00<00:00, 12554.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 11038/11038 [00:00<00:00, 12436.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2856 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 2000/2856 [00:00<00:00, 12732.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2856/2856 [00:00<00:00, 12547.38 examples/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/2070 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable\u001b[0m\n",
      "\u001b[34mTOKENIZERS_PARALLELISM\u001b[0m\n",
      "\u001b[34m=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 1/2070 [00:00<21:42,  1.59it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 2/2070 [00:00<12:56,  2.66it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 3/2070 [00:00<09:18,  3.70it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 4/2070 [00:01<07:35,  4.54it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 5/2070 [00:01<06:36,  5.20it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 6/2070 [00:01<06:02,  5.70it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 7/2070 [00:01<05:40,  6.06it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 8/2070 [00:01<05:26,  6.31it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 9/2070 [00:01<05:16,  6.52it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 10/2070 [00:01<05:08,  6.67it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 11/2070 [00:02<05:04,  6.76it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 12/2070 [00:02<05:00,  6.84it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 13/2070 [00:02<04:59,  6.88it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 14/2070 [00:02<04:57,  6.91it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 15/2070 [00:02<04:56,  6.94it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 16/2070 [00:02<04:54,  6.96it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 17/2070 [00:02<04:54,  6.96it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 18/2070 [00:03<04:54,  6.98it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 19/2070 [00:03<04:53,  6.99it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 20/2070 [00:03<04:53,  6.99it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 21/2070 [00:03<04:53,  6.99it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 22/2070 [00:03<04:50,  7.05it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 23/2070 [00:03<04:47,  7.12it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 24/2070 [00:03<04:49,  7.08it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 25/2070 [00:04<04:50,  7.04it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 26/2070 [00:04<04:50,  7.03it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 27/2070 [00:04<04:50,  7.02it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 28/2070 [00:04<04:51,  7.00it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 29/2070 [00:04<04:51,  6.99it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 30/2070 [00:04<04:51,  6.99it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 31/2070 [00:04<04:51,  6.99it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 32/2070 [00:05<04:51,  6.99it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 33/2070 [00:05<04:52,  6.97it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 34/2070 [00:05<04:53,  6.95it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 35/2070 [00:05<04:52,  6.95it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 36/2070 [00:05<04:51,  6.97it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 37/2070 [00:05<04:51,  6.97it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 38/2070 [00:05<04:51,  6.97it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 39/2070 [00:06<04:51,  6.97it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/2070 [00:06<04:53,  6.91it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 41/2070 [00:06<04:54,  6.88it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 42/2070 [00:06<04:55,  6.86it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 43/2070 [00:06<04:56,  6.84it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 44/2070 [00:06<04:53,  6.90it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 45/2070 [00:06<04:52,  6.92it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 46/2070 [00:07<04:51,  6.94it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 47/2070 [00:07<04:50,  6.96it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 48/2070 [00:07<04:50,  6.97it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 49/2070 [00:07<04:49,  6.98it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2070 [00:07<04:48,  7.00it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.125, 'learning_rate': 3.0868167202572353e-06, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2070 [00:07<04:48,  7.00it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 0/179 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/179 [00:00<00:03, 49.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 11/179 [00:00<00:03, 45.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 16/179 [00:00<00:03, 44.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 21/179 [00:00<00:03, 44.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 26/179 [00:00<00:03, 43.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 31/179 [00:00<00:03, 43.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 36/179 [00:00<00:03, 43.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 41/179 [00:00<00:03, 43.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 46/179 [00:01<00:03, 43.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 51/179 [00:01<00:02, 43.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m31%|███▏      | 56/179 [00:01<00:02, 43.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 61/179 [00:01<00:02, 43.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 66/179 [00:01<00:02, 43.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|███▉      | 71/179 [00:01<00:02, 43.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 76/179 [00:01<00:02, 43.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 81/179 [00:01<00:02, 43.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 86/179 [00:01<00:02, 43.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 91/179 [00:02<00:02, 43.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 96/179 [00:02<00:01, 42.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 101/179 [00:02<00:01, 42.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 106/179 [00:02<00:01, 42.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 111/179 [00:02<00:01, 42.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 116/179 [00:02<00:01, 43.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 121/179 [00:02<00:01, 43.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 126/179 [00:02<00:01, 43.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 131/179 [00:03<00:01, 43.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 136/179 [00:03<00:00, 43.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 141/179 [00:03<00:00, 43.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 146/179 [00:03<00:00, 43.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 151/179 [00:03<00:00, 43.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 156/179 [00:03<00:00, 43.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 161/179 [00:03<00:00, 42.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 166/179 [00:03<00:00, 42.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 171/179 [00:03<00:00, 42.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 176/179 [00:04<00:00, 43.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5872573852539062, 'eval_accuracy': 0.7909663865546218, 'eval_f1': 0.8148837209302325, 'eval_precision': 0.9947009841029523, 'eval_recall': 0.6901260504201681, 'eval_runtime': 4.3025, 'eval_samples_per_second': 663.806, 'eval_steps_per_second': 41.604, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2070 [00:12<04:48,  7.00it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 179/179 [00:04<00:00, 43.21it/s]#033[A\u001b[0m\n",
      "\u001b[34mWarning: Large gap between train and eval loss: inf\u001b[0m\n",
      "\u001b[34mConsider increasing dropout or weight decay\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 51/2070 [00:14<1:07:52,  2.02s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 52/2070 [00:14<48:59,  1.46s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 53/2070 [00:14<35:45,  1.06s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 54/2070 [00:14<26:30,  1.27it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 55/2070 [00:14<20:01,  1.68it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 56/2070 [00:14<15:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 57/2070 [00:14<12:18,  2.73it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 58/2070 [00:15<10:04,  3.33it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 59/2070 [00:15<08:30,  3.94it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2070 [00:15<07:25,  4.51it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 61/2070 [00:15<06:39,  5.03it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 62/2070 [00:15<06:07,  5.47it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 63/2070 [00:15<05:44,  5.83it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 64/2070 [00:15<05:28,  6.10it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 65/2070 [00:16<05:17,  6.32it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 66/2070 [00:16<05:11,  6.43it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 67/2070 [00:16<05:05,  6.56it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 68/2070 [00:16<05:00,  6.65it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 69/2070 [00:16<04:57,  6.72it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2070 [00:16<04:55,  6.76it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 71/2070 [00:17<04:53,  6.81it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 72/2070 [00:17<04:53,  6.81it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 73/2070 [00:17<04:52,  6.83it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 74/2070 [00:17<04:51,  6.84it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 75/2070 [00:17<04:51,  6.84it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 76/2070 [00:17<04:51,  6.84it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 77/2070 [00:17<04:50,  6.85it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 78/2070 [00:18<04:50,  6.86it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 79/2070 [00:18<04:48,  6.90it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 80/2070 [00:18<04:46,  6.93it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 81/2070 [00:18<04:46,  6.94it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 82/2070 [00:18<04:45,  6.96it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 83/2070 [00:18<04:45,  6.97it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 84/2070 [00:18<04:44,  6.99it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 85/2070 [00:19<04:43,  6.99it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 86/2070 [00:19<04:43,  7.00it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 87/2070 [00:19<04:43,  6.99it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 88/2070 [00:19<04:43,  7.00it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 89/2070 [00:19<04:42,  7.00it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2070 [00:19<04:42,  7.01it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 91/2070 [00:19<04:42,  7.00it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 92/2070 [00:20<04:42,  7.01it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 93/2070 [00:20<04:42,  7.00it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 94/2070 [00:20<04:42,  6.99it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 95/2070 [00:20<04:44,  6.95it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 96/2070 [00:20<04:45,  6.92it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 97/2070 [00:20<04:44,  6.93it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 98/2070 [00:20<04:44,  6.93it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 99/2070 [00:21<04:45,  6.91it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 100/2070 [00:21<04:44,  6.93it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3181, 'learning_rate': 6.302250803858521e-06, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m5%|▍         | 100/2070 [00:21<04:44,  6.93it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 0/179 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/179 [00:00<00:03, 53.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/179 [00:00<00:03, 47.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 17/179 [00:00<00:03, 46.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 22/179 [00:00<00:03, 44.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 27/179 [00:00<00:03, 44.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 32/179 [00:00<00:03, 44.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 37/179 [00:00<00:03, 43.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 42/179 [00:00<00:03, 43.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 47/179 [00:01<00:03, 43.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 52/179 [00:01<00:02, 43.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 57/179 [00:01<00:02, 43.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 62/179 [00:01<00:02, 42.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 67/179 [00:01<00:02, 42.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 72/179 [00:01<00:02, 42.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 77/179 [00:01<00:02, 42.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 82/179 [00:01<00:02, 42.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 87/179 [00:01<00:02, 42.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 92/179 [00:02<00:02, 42.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 97/179 [00:02<00:01, 42.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 102/179 [00:02<00:01, 42.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 107/179 [00:02<00:01, 42.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 112/179 [00:02<00:01, 42.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 117/179 [00:02<00:01, 42.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 122/179 [00:02<00:01, 42.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 127/179 [00:02<00:01, 42.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 132/179 [00:03<00:01, 42.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 137/179 [00:03<00:00, 43.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 142/179 [00:03<00:00, 43.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 147/179 [00:03<00:00, 43.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 152/179 [00:03<00:00, 43.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 157/179 [00:03<00:00, 43.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 162/179 [00:03<00:00, 43.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 167/179 [00:03<00:00, 43.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 172/179 [00:03<00:00, 43.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 177/179 [00:04<00:00, 44.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4214123785495758, 'eval_accuracy': 0.9072128851540616, 'eval_f1': 0.925331079177233, 'eval_precision': 0.998176291793313, 'eval_recall': 0.8623949579831933, 'eval_runtime': 4.2626, 'eval_samples_per_second': 670.007, 'eval_steps_per_second': 41.993, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m5%|▍         | 100/2070 [00:25<04:44,  6.93it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 179/179 [00:04<00:00, 44.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mWarning: Large gap between train and eval loss: inf\u001b[0m\n",
      "\u001b[34mConsider increasing dropout or weight decay\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m5%|▍         | 101/2070 [00:27<1:06:52,  2.04s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 102/2070 [00:27<48:14,  1.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 103/2070 [00:27<35:12,  1.07s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 104/2070 [00:28<26:03,  1.26it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 105/2070 [00:28<19:43,  1.66it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 106/2070 [00:28<15:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 107/2070 [00:28<12:14,  2.67it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 108/2070 [00:28<10:03,  3.25it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 109/2070 [00:28<08:35,  3.80it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 110/2070 [00:29<07:30,  4.35it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 111/2070 [00:29<06:43,  4.85it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 112/2070 [00:29<06:10,  5.28it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 113/2070 [00:29<05:47,  5.63it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 114/2070 [00:29<05:38,  5.78it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 115/2070 [00:29<05:31,  5.89it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 116/2070 [00:29<05:33,  5.85it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 117/2070 [00:30<05:21,  6.07it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 118/2070 [00:30<05:22,  6.06it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 119/2070 [00:30<05:14,  6.20it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 120/2070 [00:30<05:05,  6.39it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 121/2070 [00:30<04:58,  6.53it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 122/2070 [00:30<04:53,  6.63it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 123/2070 [00:31<04:50,  6.71it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 124/2070 [00:31<04:47,  6.77it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 125/2070 [00:31<04:47,  6.76it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 126/2070 [00:31<04:45,  6.80it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 127/2070 [00:31<04:44,  6.82it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 128/2070 [00:31<04:46,  6.79it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 129/2070 [00:31<04:44,  6.82it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 130/2070 [00:32<04:43,  6.85it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 131/2070 [00:32<04:41,  6.88it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 132/2070 [00:32<04:40,  6.91it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 133/2070 [00:32<04:39,  6.92it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 134/2070 [00:32<04:39,  6.94it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 135/2070 [00:32<04:38,  6.95it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 136/2070 [00:32<04:37,  6.97it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 137/2070 [00:33<04:37,  6.97it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 138/2070 [00:33<04:36,  6.98it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 139/2070 [00:33<04:39,  6.92it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 140/2070 [00:33<04:38,  6.93it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 141/2070 [00:33<04:37,  6.95it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 142/2070 [00:33<04:36,  6.96it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 143/2070 [00:33<04:36,  6.96it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 144/2070 [00:34<04:36,  6.97it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 145/2070 [00:34<04:35,  6.98it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 146/2070 [00:34<04:35,  6.98it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 147/2070 [00:34<04:35,  6.99it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 148/2070 [00:34<04:35,  6.98it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 149/2070 [00:34<04:35,  6.98it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 150/2070 [00:34<04:34,  6.98it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2524, 'learning_rate': 9.517684887459809e-06, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m7%|▋         | 150/2070 [00:34<04:34,  6.98it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 0/179 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/179 [00:00<00:03, 51.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/179 [00:00<00:03, 46.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 17/179 [00:00<00:03, 45.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 22/179 [00:00<00:03, 44.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 27/179 [00:00<00:03, 44.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 32/179 [00:00<00:03, 44.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 37/179 [00:00<00:03, 44.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 42/179 [00:00<00:03, 44.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 47/179 [00:01<00:02, 44.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 52/179 [00:01<00:02, 44.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 57/179 [00:01<00:02, 44.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 62/179 [00:01<00:02, 44.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 67/179 [00:01<00:02, 44.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 72/179 [00:01<00:02, 44.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 77/179 [00:01<00:02, 44.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 82/179 [00:01<00:02, 44.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 87/179 [00:01<00:02, 44.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 92/179 [00:02<00:01, 44.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 97/179 [00:02<00:01, 44.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 102/179 [00:02<00:01, 44.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 107/179 [00:02<00:01, 43.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 112/179 [00:02<00:01, 43.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 117/179 [00:02<00:01, 43.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 122/179 [00:02<00:01, 43.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 127/179 [00:02<00:01, 42.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 132/179 [00:02<00:01, 43.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 137/179 [00:03<00:00, 43.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 142/179 [00:03<00:00, 43.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 147/179 [00:03<00:00, 42.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 152/179 [00:03<00:00, 42.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 157/179 [00:03<00:00, 42.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 162/179 [00:03<00:00, 43.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 167/179 [00:03<00:00, 43.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 172/179 [00:03<00:00, 44.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 177/179 [00:04<00:00, 45.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4095214605331421, 'eval_accuracy': 0.9138655462184874, 'eval_f1': 0.9310538116591929, 'eval_precision': 0.9981971153846154, 'eval_recall': 0.8723739495798319, 'eval_runtime': 4.2025, 'eval_samples_per_second': 679.6, 'eval_steps_per_second': 42.594, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m7%|▋         | 150/2070 [00:39<04:34,  6.98it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 179/179 [00:04<00:00, 45.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mWarning: Large gap between train and eval loss: inf\u001b[0m\n",
      "\u001b[34mConsider increasing dropout or weight decay\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 151/2070 [00:41<1:05:28,  2.05s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 152/2070 [00:41<47:10,  1.48s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 153/2070 [00:41<34:23,  1.08s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 154/2070 [00:41<25:27,  1.25it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 155/2070 [00:41<19:11,  1.66it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 156/2070 [00:42<14:48,  2.15it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 157/2070 [00:42<11:43,  2.72it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 158/2070 [00:42<09:35,  3.32it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 159/2070 [00:42<08:04,  3.94it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 160/2070 [00:42<07:02,  4.52it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 161/2070 [00:42<06:17,  5.06it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 162/2070 [00:42<05:46,  5.50it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 163/2070 [00:43<05:24,  5.87it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 164/2070 [00:43<05:09,  6.15it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 165/2070 [00:43<04:58,  6.38it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 166/2070 [00:43<04:51,  6.53it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 167/2070 [00:43<04:46,  6.64it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 168/2070 [00:43<04:42,  6.72it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 169/2070 [00:43<04:39,  6.80it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 170/2070 [00:44<04:37,  6.84it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 171/2070 [00:44<04:35,  6.90it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 172/2070 [00:44<04:33,  6.93it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 173/2070 [00:44<04:32,  6.97it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 174/2070 [00:44<04:34,  6.91it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 175/2070 [00:44<04:36,  6.85it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 176/2070 [00:45<04:38,  6.81it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 177/2070 [00:45<04:38,  6.80it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 178/2070 [00:45<04:37,  6.81it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 179/2070 [00:45<04:37,  6.81it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 180/2070 [00:45<04:35,  6.87it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 181/2070 [00:45<04:33,  6.89it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 182/2070 [00:45<04:34,  6.87it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 183/2070 [00:46<04:34,  6.86it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 184/2070 [00:46<04:36,  6.82it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 185/2070 [00:46<04:35,  6.84it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 186/2070 [00:46<04:35,  6.83it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 187/2070 [00:46<04:35,  6.84it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 188/2070 [00:46<04:35,  6.82it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 189/2070 [00:46<04:33,  6.87it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 190/2070 [00:47<04:33,  6.88it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 191/2070 [00:47<04:32,  6.90it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 192/2070 [00:47<04:31,  6.92it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 193/2070 [00:47<04:31,  6.92it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 194/2070 [00:47<04:31,  6.91it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 195/2070 [00:47<04:31,  6.90it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 196/2070 [00:47<04:30,  6.92it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 197/2070 [00:48<04:30,  6.94it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 198/2070 [00:48<04:29,  6.94it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 199/2070 [00:48<04:29,  6.95it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 200/2070 [00:48<04:29,  6.94it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2421, 'learning_rate': 1.2668810289389068e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m10%|▉         | 200/2070 [00:48<04:29,  6.94it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 0/179 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/179 [00:00<00:03, 53.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/179 [00:00<00:03, 47.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 17/179 [00:00<00:03, 46.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 22/179 [00:00<00:03, 45.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 27/179 [00:00<00:03, 44.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 32/179 [00:00<00:03, 44.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 37/179 [00:00<00:03, 44.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 42/179 [00:00<00:03, 44.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 47/179 [00:01<00:02, 44.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 52/179 [00:01<00:02, 44.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 57/179 [00:01<00:02, 44.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 62/179 [00:01<00:02, 44.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 67/179 [00:01<00:02, 44.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 72/179 [00:01<00:02, 44.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 77/179 [00:01<00:02, 44.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 82/179 [00:01<00:02, 44.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 87/179 [00:01<00:02, 44.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 92/179 [00:02<00:01, 44.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 97/179 [00:02<00:01, 44.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 102/179 [00:02<00:01, 43.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 107/179 [00:02<00:01, 43.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 112/179 [00:02<00:01, 43.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 117/179 [00:02<00:01, 43.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 122/179 [00:02<00:01, 43.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 127/179 [00:02<00:01, 43.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 132/179 [00:02<00:01, 43.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 137/179 [00:03<00:00, 43.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 142/179 [00:03<00:00, 43.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 147/179 [00:03<00:00, 43.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 152/179 [00:03<00:00, 43.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 157/179 [00:03<00:00, 43.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 162/179 [00:03<00:00, 44.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 167/179 [00:03<00:00, 44.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 172/179 [00:03<00:00, 44.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 177/179 [00:03<00:00, 45.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.41325467824935913, 'eval_accuracy': 0.8998599439775911, 'eval_f1': 0.9188882586500283, 'eval_precision': 0.998766954377312, 'eval_recall': 0.8508403361344538, 'eval_runtime': 4.1688, 'eval_samples_per_second': 685.09, 'eval_steps_per_second': 42.938, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m10%|▉         | 200/2070 [00:52<04:29,  6.94it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 179/179 [00:04<00:00, 45.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mWarning: Large gap between train and eval loss: inf\u001b[0m\n",
      "\u001b[34mConsider increasing dropout or weight decay\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 201/2070 [00:54<1:02:57,  2.02s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 202/2070 [00:55<45:22,  1.46s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 203/2070 [00:55<33:04,  1.06s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 204/2070 [00:55<24:28,  1.27it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 205/2070 [00:55<18:26,  1.69it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 206/2070 [00:55<14:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 207/2070 [00:55<11:17,  2.75it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 208/2070 [00:55<09:13,  3.37it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 209/2070 [00:56<07:46,  3.99it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 210/2070 [00:56<06:45,  4.58it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 211/2070 [00:56<06:03,  5.12it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 212/2070 [00:56<05:33,  5.57it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 213/2070 [00:56<05:12,  5.94it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 214/2070 [00:56<04:59,  6.21it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 215/2070 [00:56<04:50,  6.38it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 216/2070 [00:57<04:45,  6.50it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 217/2070 [00:57<04:40,  6.60it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 218/2070 [00:57<04:37,  6.68it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 219/2070 [00:57<04:34,  6.74it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 220/2070 [00:57<04:33,  6.77it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 221/2070 [00:57<04:31,  6.80it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 222/2070 [00:57<04:31,  6.81it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 223/2070 [00:58<04:30,  6.83it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 224/2070 [00:58<04:29,  6.84it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 225/2070 [00:58<04:28,  6.86it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 226/2070 [00:58<04:28,  6.87it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 227/2070 [00:58<04:27,  6.89it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 228/2070 [00:58<04:27,  6.88it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 229/2070 [00:58<04:27,  6.89it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 230/2070 [00:59<04:27,  6.88it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 231/2070 [00:59<04:26,  6.89it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 232/2070 [00:59<04:25,  6.93it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 233/2070 [00:59<04:24,  6.95it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 234/2070 [00:59<04:24,  6.95it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 235/2070 [00:59<04:24,  6.93it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 236/2070 [00:59<04:25,  6.91it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 237/2070 [01:00<04:25,  6.89it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 238/2070 [01:00<04:25,  6.89it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 239/2070 [01:00<04:25,  6.90it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 240/2070 [01:00<04:25,  6.89it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 241/2070 [01:00<04:25,  6.89it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 242/2070 [01:00<04:25,  6.90it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 243/2070 [01:00<04:25,  6.89it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 244/2070 [01:01<04:25,  6.89it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 245/2070 [01:01<04:25,  6.88it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 246/2070 [01:01<04:25,  6.88it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 247/2070 [01:01<04:25,  6.88it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 248/2070 [01:01<04:24,  6.88it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 249/2070 [01:01<04:26,  6.84it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 250/2070 [01:01<04:25,  6.85it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2295, 'learning_rate': 1.5884244372990355e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 250/2070 [01:01<04:25,  6.85it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 0/179 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/179 [00:00<00:03, 51.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/179 [00:00<00:03, 46.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 17/179 [00:00<00:03, 44.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 22/179 [00:00<00:03, 43.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 27/179 [00:00<00:03, 43.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 32/179 [00:00<00:03, 43.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 37/179 [00:00<00:03, 43.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 42/179 [00:00<00:03, 43.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 47/179 [00:01<00:03, 43.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 52/179 [00:01<00:02, 43.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 57/179 [00:01<00:02, 42.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 62/179 [00:01<00:02, 42.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 67/179 [00:01<00:02, 42.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 72/179 [00:01<00:02, 43.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 77/179 [00:01<00:02, 43.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 82/179 [00:01<00:02, 43.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 87/179 [00:01<00:02, 44.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 92/179 [00:02<00:01, 44.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 97/179 [00:02<00:01, 44.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 102/179 [00:02<00:01, 43.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 107/179 [00:02<00:01, 43.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 112/179 [00:02<00:01, 43.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 117/179 [00:02<00:01, 43.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 122/179 [00:02<00:01, 43.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 127/179 [00:02<00:01, 43.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 132/179 [00:03<00:01, 43.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 137/179 [00:03<00:00, 43.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 142/179 [00:03<00:00, 43.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 147/179 [00:03<00:00, 43.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 152/179 [00:03<00:00, 43.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 157/179 [00:03<00:00, 43.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 162/179 [00:03<00:00, 43.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 167/179 [00:03<00:00, 43.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 172/179 [00:03<00:00, 43.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 177/179 [00:04<00:00, 44.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.38757094740867615, 'eval_accuracy': 0.9124649859943977, 'eval_f1': 0.9298147108366086, 'eval_precision': 0.9987937273823885, 'eval_recall': 0.8697478991596639, 'eval_runtime': 4.2373, 'eval_samples_per_second': 674.019, 'eval_steps_per_second': 42.244, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 250/2070 [01:06<04:25,  6.85it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 179/179 [00:04<00:00, 44.52it/s]#033[A\u001b[0m\n",
      "\u001b[34mWarning: Large gap between train and eval loss: inf\u001b[0m\n",
      "\u001b[34mConsider increasing dropout or weight decay\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 251/2070 [01:08<1:01:56,  2.04s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 252/2070 [01:08<44:38,  1.47s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 253/2070 [01:08<32:30,  1.07s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 254/2070 [01:08<24:03,  1.26it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 255/2070 [01:09<18:07,  1.67it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 256/2070 [01:09<13:59,  2.16it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 257/2070 [01:09<11:04,  2.73it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 258/2070 [01:09<09:02,  3.34it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 259/2070 [01:09<07:36,  3.96it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 260/2070 [01:09<06:37,  4.55it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 261/2070 [01:09<05:55,  5.09it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 262/2070 [01:10<05:26,  5.53it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 263/2070 [01:10<05:06,  5.90it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 264/2070 [01:10<04:52,  6.18it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 265/2070 [01:10<04:41,  6.41it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 266/2070 [01:10<04:34,  6.57it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 267/2070 [01:10<04:29,  6.70it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 268/2070 [01:10<04:26,  6.77it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 269/2070 [01:11<04:23,  6.84it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 270/2070 [01:11<04:24,  6.81it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 271/2070 [01:11<04:23,  6.82it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 272/2070 [01:11<04:24,  6.80it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 273/2070 [01:11<04:23,  6.83it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 274/2070 [01:11<04:22,  6.85it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 275/2070 [01:11<04:20,  6.89it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 276/2070 [01:12<04:19,  6.91it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 277/2070 [01:12<04:18,  6.93it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 278/2070 [01:12<04:18,  6.93it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 279/2070 [01:12<04:18,  6.92it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 280/2070 [01:12<04:18,  6.92it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 281/2070 [01:12<04:18,  6.92it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 282/2070 [01:12<04:18,  6.91it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 283/2070 [01:13<04:17,  6.93it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 284/2070 [01:13<04:17,  6.94it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 285/2070 [01:13<04:16,  6.95it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 286/2070 [01:13<04:16,  6.95it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 287/2070 [01:13<04:16,  6.95it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 288/2070 [01:13<04:16,  6.96it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 289/2070 [01:13<04:16,  6.93it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 290/2070 [01:14<04:19,  6.86it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 291/2070 [01:14<04:19,  6.85it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 292/2070 [01:14<04:19,  6.84it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 293/2070 [01:14<04:20,  6.83it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 294/2070 [01:14<04:20,  6.83it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 295/2070 [01:14<04:20,  6.82it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 296/2070 [01:14<04:20,  6.82it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 297/2070 [01:15<04:20,  6.81it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 298/2070 [01:15<04:20,  6.80it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 299/2070 [01:15<04:20,  6.79it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 300/2070 [01:15<04:20,  6.80it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2245, 'learning_rate': 1.9099678456591642e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 300/2070 [01:15<04:20,  6.80it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 0/179 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/179 [00:00<00:03, 50.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/179 [00:00<00:03, 45.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 17/179 [00:00<00:03, 44.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 22/179 [00:00<00:03, 43.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 27/179 [00:00<00:03, 43.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 32/179 [00:00<00:03, 43.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 37/179 [00:00<00:03, 43.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 42/179 [00:00<00:03, 42.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▋       | 47/179 [00:01<00:03, 42.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 52/179 [00:01<00:02, 42.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 57/179 [00:01<00:02, 42.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 62/179 [00:01<00:02, 42.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 67/179 [00:01<00:02, 42.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 72/179 [00:01<00:02, 42.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 77/179 [00:01<00:02, 42.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 82/179 [00:01<00:02, 42.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▊     | 87/179 [00:02<00:02, 42.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 92/179 [00:02<00:02, 42.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 97/179 [00:02<00:01, 42.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 102/179 [00:02<00:01, 42.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 107/179 [00:02<00:01, 42.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 112/179 [00:02<00:01, 42.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 117/179 [00:02<00:01, 42.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 122/179 [00:02<00:01, 42.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 127/179 [00:02<00:01, 42.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 132/179 [00:03<00:01, 42.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 137/179 [00:03<00:00, 43.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 142/179 [00:03<00:00, 43.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 147/179 [00:03<00:00, 43.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 152/179 [00:03<00:00, 43.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 157/179 [00:03<00:00, 43.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 162/179 [00:03<00:00, 43.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 167/179 [00:03<00:00, 43.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 172/179 [00:03<00:00, 43.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 177/179 [00:04<00:00, 44.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.39600053429603577, 'eval_accuracy': 0.9121148459383753, 'eval_f1': 0.9295141814097164, 'eval_precision': 0.9987929993964997, 'eval_recall': 0.8692226890756303, 'eval_runtime': 4.3331, 'eval_samples_per_second': 659.117, 'eval_steps_per_second': 41.31, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 300/2070 [01:19<04:20,  6.80it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 179/179 [00:04<00:00, 44.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mWarning: Large gap between train and eval loss: inf\u001b[0m\n",
      "\u001b[34mConsider increasing dropout or weight decay\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0.87 completed.\u001b[0m\n",
      "\u001b[34mEval Loss: 0.3960\u001b[0m\n",
      "\u001b[34mEval Accuracy: 0.9121\u001b[0m\n",
      "\u001b[34mEval F1: 0.9295\u001b[0m\n",
      "\u001b[34mEval Precision: 0.9988\u001b[0m\n",
      "\u001b[34mEval Recall: 0.8692\u001b[0m\n",
      "\u001b[34m{'train_runtime': 82.2015, 'train_samples_per_second': 805.678, 'train_steps_per_second': 25.182, 'train_loss': 0.3985866610209147, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 300/2070 [01:22<04:20,  6.80it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 300/2070 [01:22<08:04,  3.65it/s]\u001b[0m\n",
      "\u001b[34m2024-11-08 16:01:56,877 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-08 16:01:56,877 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-08 16:01:56,878 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-11-08 16:02:00 Uploading - Uploading generated training model\n",
      "2024-11-08 16:02:48 Completed - Training job completed\n",
      "Training seconds: 433\n",
      "Billable seconds: 433\n"
     ]
    }
   ],
   "source": [
    "from prepare import prepare_data, setup_training\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 首先准备数据\n",
    "    # print(\"准备训练数据...\")\n",
    "    # train_path, eval_path = prepare_data()\n",
    "\n",
    "    # 2. 设置并启动训练\n",
    "    print(\"开始训练任务...\")\n",
    "    estimator = setup_training()\n",
    "    #\n",
    "    # # 3. 部署模型（可选）\n",
    "    # print(\"部署模型...\")\n",
    "    # predictor = estimator.deploy(\n",
    "    #     initial_instance_count=1,\n",
    "    #     instance_type='ml.m5.xlarge'\n",
    "    # )\n",
    "\n",
    "    # # 4. 测试预测（可选）\n",
    "    # test_text = \"测试文本\"\n",
    "    # prediction = predictor.predict({\n",
    "    #     \"inputs\": test_text\n",
    "    # })\n",
    "    # print(\"预测结果:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04cedec-ad8c-4eae-bf09-ecd0e88fe1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.11/site-packages (0.42.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install jieba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
